Applications_04.html:  </style></head><body><div class="content"><h1>Speaker-dependent Voice Command Recognition</h1><!--introduction--><p>In this application showcase, we shall use DTW (dynamic time warping) for speaker-dependent voice command recognition. This is the simplest voice command recognition system, in which the user's utterance is compared with those pre-recorded utterances (by the same user) using DTW. The utterance with the shortest DTW distance is identified, and the corresponding action will be triggered as the result of the voice command system.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Basics</a></li><li><a href="#2">Reference</a></li></ul></div><h2>Basics<a name="1"></a></h2><p>There are two stages involved in building such a system:</p><div><ul><li>Registration (or Enrollment): Each user is required to record several utterances (or spoken keywords) as the database</li><li>Recognition: During application, the user should say a keyword to a microphone, and the system can identify the closest commands in the database.</li></ul></div><p>When comparing the user's input to the recordings in the database, we use DTW for elastic comparison since it tolerates variations in speech rate. Each recording should go through endpoint detection to remove leading and trailing silence. The utterance is then chopped into frames, and each frame is converted into a 39-dim feature vector call MFCC (Mel-scale frequency cepstral coefficient). DTW is then employed to compute the distance betweens two sets of MFCCs extracted from two utterances.</p><p>The demo is available within the toolbox folder "demo/sdVoiceCommand". There are two major commands:</p><div><ul><li>goRegister.m: Run this script to do registration. In particular, you need to say "one", "two", "three", "four", "five" sequentially to build the database. If you need to change these commands, modify the value of the variable "sentence". Each utterance will go through endpoint detection and feature extraction. The features together with the recording sp
Applications_04.html:% speaker-dependent voice command recognition. This is the simplest voice
Applications_04.html:% command recognition system, in which the user's utterance is compared
Applications_04.html:% corresponding action will be triggered as the result of the voice command system. 
Applications_04.html:% * Recognition: During application, the user should say a keyword to a microphone, and the system can identify the closest commands in the database.
Applications_04.html:% There are two major commands:
Applications_04.html:% * goRegister.m: Run this script to do registration. In particular, you need to say "one", "two", "three", "four", "five" sequentially to build the database. If you need to change these commands, modify the value of the variable "sentence". Each utterance will go through endpoint detection and feature extraction. The features together with the recording specs are saved at mfcc4dtwDemo.mat. 
dtwPathPlot_help.html:<p>Note that dtwPath must be obtained in advance, by any one of the DTW commands in the toolbox.
dtwPathPlot_help.html:% <p>Note that dtwPath must be obtained in advance, by any one of the DTW commands in the toolbox.
index.html:<li><a target=_blank href="mltDoc_help.html">mltDoc</a>: Online document of the given MLT command
mltDoc_help.html:  </style></head><body><div class="content"><h1>mltDoc</h1><!--introduction--><p>Online document of the given MLT command</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Syntax</a></li><li><a href="#2">Description</a></li><li><a href="#3">Example</a></li></ul></div><h2>Syntax<a name="1"></a></h2><div><ul><li>mltDoc(mltCommand)</li></ul></div><h2>Description<a name="2"></a></h2><p>mltDoc(mltCommand) brings up the help browser which displays the online document of the givem MLT command.</p><h2>Example<a name="3"></a></h2><pre class="codeinput">mltDoc(<span class="string">'pca'</span>)
mltDoc_help.html:% Online document of the given MLT command
mltDoc_help.html:% 		mltDoc(mltCommand) brings up the help browser which displays the online document of the givem MLT command.
mlt_functions_by_cat.html:% * <mltDoc_help.html |mltDoc|> - Online document of the given MLT command
mlt_getting_started.html:</pre><img vspace="5" hspace="5" src="mlt_getting_started_04.png" alt=""> <p>The scatter plots show the data distribution after projecting onto 3 features. Since our visual preception is based 2D, the "feel" of the distribution may depend on your viewing angles. Fortunately you can click and drag each plot to change the viewing angle.</p><p>In fact, you can use LDA (linear discriminant analysis) to find the best projection for classification. The command is "lda".</p><h2>Classifier Design<a name="8"></a></h2><p>Once we have the dataset, we can design a classifier and evaluate its performance. For simplicity, we shall design a naive Bayes classifier (NBC) for the Iris dataset. For easy visualization, we shall only use inputs 3 and 4 of the dataset for classification.</p><p>As a common practice in pattern recognition, we need to partition the dataset into a training set and a test set. Usually we use the training set for designing a classifier, and the test set for evaluate the performance of the classifier. We can use the command cvDataGen to create these two sets:</p><pre class="codeinput">DS.input=DS.input(3:4, :);			<span class="comment">% Only take dimensions 3 and 4 for 2d visualization</span>
mlt_getting_started.html:</pre><p>Note that cvDataGen is commonly used for generating datasets for m-fold cross-validation as a more precise estimate of the recognition rate. But here we are only m=2 to obtain the training and test sets.</p><p>Now we can use TS for designing a classifier, and VS for performance evaluation. Here we use the naive Bayes classifier in this guided tour. We use the command nbcTrain to train the classifier:</p><pre class="codeinput">[nbcPrm, logLike, recogRate, hitIndex]=nbcTrain(TS);
mlt_getting_started.html:</pre><img vspace="5" hspace="5" src="mlt_getting_started_07.png" alt=""> <p>Note that the second input argument to nbcTrain is used to specify the parameters for training the NBC. Here we used an empty matrix to indicate that default parameters are adopted for training. The third nonzero input argument requests the command to perform more visualization, including</p><div><ul><li>Display the scatter plot</li><li>Draw the decision boundaries of these 3 classes</li><li>Put a cross on the misclassified instances</li></ul></div><h2>PDF Plots<a name="15"></a></h2><p>In NBC training, we assume that the PDF (probability density function) for each class is obtained as the product of the PDFs over all input dimensions. To plot the PDF of each class along each dimension, you can use the command nbcPlot:</p><pre class="codeinput">nbcPlot(TS, nbcPrm, <span class="string">'1dPdf'</span>);
mlt_getting_started.html:% projection for classification. The command is "lda".
mlt_getting_started.html:% performance of the classifier. We can use the command cvDataGen to create
mlt_getting_started.html:% We use the command nbcTrain to train the classifier:
mlt_getting_started.html:% argument requests the command to perform more visualization, including
mlt_getting_started.html:% dimensions. To plot the PDF of each class along each dimension, you can use the command nbcPlot:
nbcEval_help.html:  </style></head><body><div class="content"><h1>nbcEval</h1><!--introduction--><p>Evaluation for the NBC (naive bayes classifier)</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Syntax</a></li><li><a href="#2">Description</a></li><li><a href="#3">Example</a></li><li><a href="#5">See Also</a></li></ul></div><h2>Syntax<a name="1"></a></h2><div><ul><li>[computedClass, logLike, recogRate, hitIndex]=nbcEval(DS, nbcPrm, plotOpt)</li><li>If DS does not have "output" field, then this command won't return "recogRate" and "hitIndex".</li></ul></div><h2>Description<a name="2"></a></h2><p>[computedClass, logLike, recogRate, hitIndex]=nbcEval(DS, nbcPrm, plotOpt) returns the evaluation results of NBC</p><h2>Example<a name="3"></a></h2><pre class="codeinput">DS=prData(<span class="string">'iris'</span>);
nbcEval_help.html:% * 		If DS does not have "output" field, then this command won't return "recogRate" and "hitIndex".
polarTransform_help.html:<p>command (ij mode).
polarTransform_help.html:% <p>command (ij mode).
qcEval_help.html:<p>If DS does not have "output" field, then this command won't return "recogRate" and "hitIndex".
qcEval_help.html:% <p>If DS does not have "output" field, then this command won't return "recogRate" and "hitIndex".
srcEval_help.html:  </style></head><body><div class="content"><h1>srcEval</h1><!--introduction--><p>Evaluation of SRC (sparse-representation classifier)</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Syntax</a></li><li><a href="#2">Description</a></li><li><a href="#3">Example</a></li><li><a href="#5">See Also</a></li></ul></div><h2>Syntax<a name="1"></a></h2><div><ul><li>[computedClass, invDist, recogRate, hitIndex]=srcEval(DS, srcModel)</li><li>If DS does not have "output" field, then this command won't return "recogRate" and "hitIndex".</li></ul></div><h2>Description<a name="2"></a></h2><p>[computedClass, invDist, recogRate, hitIndex]=srcEval(DS, srcModel, plotOpt) returns the evaluation results of SRC</p><h2>Example<a name="3"></a></h2><pre class="codeinput">DS=prData(<span class="string">'iris'</span>);
srcEval_help.html:% * 		If DS does not have "output" field, then this command won't return "recogRate" and "hitIndex".
tb_functions_by_cat.html:% * <mltDoc_help.html |mltDoc|> - Online document of the given MLT command
tb_getting_started.html:</pre><img vspace="5" hspace="5" src="tb_getting_started_04.png" alt=""> <p>The scatter plots show the data distribution after projecting onto 3 features. Since our visual preception is based 2D, the "feel" of the distribution may depend on your viewing angles. Fortunately you can click and drag each plot to change the viewing angle.</p><p>In fact, you can use LDA (linear discriminant analysis) to find the best projection for classification. The command is "lda".</p><h2>Classifier Design<a name="8"></a></h2><p>Once we have the dataset, we can design a classifier and evaluate its performance. For simplicity, we shall design a naive Bayes classifier (NBC) for the Iris dataset. For easy visualization, we shall only use inputs 3 and 4 of the dataset for classification.</p><p>As a common practice in pattern recognition, we need to partition the dataset into a training set and a test set. Usually we use the training set for designing a classifier, and the test set for evaluate the performance of the classifier. We can use the command cvDataGen to create these two sets:</p><pre class="codeinput">DS.input=DS.input(3:4, :);			<span class="comment">% Only take dimensions 3 and 4 for 2d visualization</span>
tb_getting_started.html:</pre><p>Note that cvDataGen is commonly used for generating datasets for m-fold cross-validation as a more precise estimate of the recognition rate. But here we are only m=2 to obtain the training and test sets.</p><p>Now we can use TS for designing a classifier, and VS for performance evaluation. Here we use the naive Bayes classifier in this guided tour. We use the command nbcTrain to train the classifier:</p><pre class="codeinput">[nbcPrm, logLike, recogRate, hitIndex]=nbcTrain(TS);
tb_getting_started.html:</pre><img vspace="5" hspace="5" src="tb_getting_started_07.png" alt=""> <p>Note that the second input argument to nbcTrain is used to specify the parameters for training the NBC. Here we used an empty matrix to indicate that default parameters are adopted for training. The third nonzero input argument requests the command to perform more visualization, including</p><div><ul><li>Display the scatter plot</li><li>Draw the decision boundaries of these 3 classes</li><li>Put a cross on the misclassified instances</li></ul></div><h2>PDF Plots<a name="15"></a></h2><p>In NBC training, we assume that the PDF (probability density function) for each class is obtained as the product of the PDFs over all input dimensions. To plot the PDF of each class along each dimension, you can use the command nbcPlot:</p><pre class="codeinput">nbcPlot(TS, nbcPrm, <span class="string">'1dPdf'</span>);
tb_getting_started.html:% projection for classification. The command is "lda".
tb_getting_started.html:% performance of the classifier. We can use the command cvDataGen to create
tb_getting_started.html:% We use the command nbcTrain to train the classifier:
tb_getting_started.html:% argument requests the command to perform more visualization, including
tb_getting_started.html:% dimensions. To plot the PDF of each class along each dimension, you can use the command nbcPlot:
userGuide_04PerformanceEvaluation_01.html:</pre><h2>M-fold cross validation<a name="4"></a></h2><p>We can extend the concept of outside test to have the so-called two-fold cross validation or two-way outside-test recognition rate. Namely, we can divide the data set into part A and B of equal size. In the first run, we use part A as the training set and part B as the test set. In the second run, we reverse the roles of part A and B. The overall recognition rate will be the average of these two outside-test recognition rates.</p><p>In two-fold cross validation, the dataset is divided into two equal-size parts, which lead to slight lower outside-test recognition rates since each classifier can only use 50% of the dataset. In order to estimate the recognition rate better, we can have m-fold cross validation in which the dataset is divided into m subsets of about equal size. Then we estimate the recognition rate according to the following steps:</p><div><ul><li>Use subset i as the test set, while all the others as the training set to design a classifier. Test the classifier using subset i to obtain the outside-test recognition rate.</li><li>Repeat the above step for each i, i = 1 to m. Compute the overall average outside-test recognition rate.</li></ul></div><p>The following example using the command crossValidate to partition the dataset into 10 folds in order to compute the 10-fold cross validation:</p><pre class="codeinput">DS=prData(<span class="string">'iris'</span>);
userGuide_04PerformanceEvaluation_01.html:</pre><img vspace="5" hspace="5" src="userGuide_04PerformanceEvaluation_01_03.png" alt=""> <p>The function knncLoo.m is efficient in computing the LOO recognition rate of 1-NNC. For a more sophisticated classifier, the LOO test is usually time consuming. By setting the no. of folds to be inf, the following example employs the command crossValidate to compute the LOO recognition rate of the Iris dataset using the quadratic classifier:</p><pre class="codeinput">DS=prData(<span class="string">'iris'</span>);
userGuide_04PerformanceEvaluation_01.html:% The following example using the command crossValidate to partition the
userGuide_04PerformanceEvaluation_01.html:% By setting the no. of folds to be inf, the following example employs the command crossValidate to compute the LOO
